{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "1. Which Linear Regression training algorithm can you use if you have a training set\n",
    "with millions of features?\n",
    "2. Can Gradient descent get stuck in a local minimum when training a logistic\n",
    "regression model?\n",
    "3. Do all Gradient Descent algorithms lead to the same model, provided they run for\n",
    "the same no. of epochs?\n",
    "4. Suppose you are using a Polynomial Regression and you plot the learning curves\n",
    "and you notice there is a large gap between training and validation error. What is\n",
    "the problem? How to solve it?\n",
    "5. Suppose the features in your training dataset are in different scales. Which\n",
    "algorithms will suffer from this? How to handle this situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use batch gradient descent, stochastic gradient descent, or mini-batch gradient descent. SGD and MBGD would work the best because neither of them need to load the entire dataset into memory in order to take 1 step of gradient descent. Batch would be ok with the caveat that you have enough memory to load all the data.\n",
    "\n",
    "The normal equations method would not be a good choice because it is computationally inefficient. The main cause of the computational complexity comes from inverse operation on an (n x n) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent produces a convex shaped graph which only has one global optimum. Therefore, it cannot get stuck in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. The issue is that stochastic gradient descent and mini-batch gradient descent have randomness built into them. This means that they can find their way to nearby the global optimum, but they generally don't converge. One way to help them converge is to gradually reduce the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you plot learning curves and notice there is a large gap between the training error and the validation error that is characteristic of an overfitting model. The \"gap\" exists simply because the training error is lower than the validation error. One way to imrpove an overfitting model is to provide more training data. Another tactic is to reduce the complexity of the model. You can also reduce the number of features in your data. One last thing to try is add regularization to your model. Either L2 (ridge regression) or L1 (lasso) are good choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the features in your training set have very different scales: what algorithms might suffer from this, and how? What can you do about it? The normal equations method does not require normalizing the features, so it remains unaffected by features in the training set having very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b89eb163356ecec32b39b5fa0afb59c6df6327d25999fa4ae2b1c90491fcb0e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
